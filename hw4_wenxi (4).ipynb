{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biostats 280 HW04\n",
    "### Wenxi Yu\n",
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Delta_d$ is the unit simplex in d dimensions, we have \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int_{\\Delta_d}\\pi(\\mathbf{p}) \\, d \\mathbf{p} &= 1\\\\\n",
    "\\pi(\\mathbf{p}) &=  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then we consider\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\int_{\\Delta_d}  \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p} &= \\int_{\\Delta_d}\\prod_{j=1}^d p_j^{x_j}  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} d \\mathbf{p}\\\\\n",
    "&= \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\int_{\\Delta_d}\\prod_{j=1}^d p_j^{x_j+\\alpha_j-1} d \\mathbf{p} \\\\\n",
    "&= \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|+|\\mathbf{x}|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}\\prod_{j=1}^d p_j^{x_j+\\alpha_j-1} d \\mathbf{p} \\\\\n",
    "&=\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}.         (|\\alpha| = \\sum_{j=1}^d \\alpha_j; |\\mathbf{x}|=\\sum_{j=1}^d x_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    " f(\\mathbf{x} \\mid \\alpha) &= f(\\mathbf{p} \\mid \\alpha) \\cdot f(\\mathbf{x} \\mid \\mathbf{p})\\\\\n",
    " &= \\int_{\\Delta_d}\\pi(\\mathbf{p})d \\mathbf{p} \\cdot \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\\\\n",
    "\t&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p} \n",
    "\\\\\n",
    "&=\\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, the log-likelihood is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha) &= \\ln \\prod_{i=1}^n  f(\\mathbf{x}_i \\mid \\alpha)\\\\\n",
    "&=\\sum_{i=1}^n\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j + x_{ij})- \\sum_{i=1}^n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j)+\\sum_{i=1}^n \\ln \\Gamma(|\\alpha|)- \\sum_{i=1}^n \\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|)\\\\\n",
    "&= \\sum_{i=1}^n\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)].\n",
    "\\end{aligned}\n",
    "$$\n",
    "Is the log-likelihood a concave function?\n",
    "\n",
    "Yes, we can prove this by calculating the Hessian of L (which can be found in details in Q5.)\n",
    "\n",
    "Observed information matrix: \n",
    "\n",
    "For $k \\neq j$, $d^2L(\\alpha)_{j, k} = -\\sum_{i = 1}^n\\Psi_1(|\\alpha|+|x_i|) +n\\Psi_1(|\\alpha|) < 0$\n",
    "\n",
    "For $k = j$, $d^2L(\\alpha)_{j, k} = \\sum_{i = 1}^n (\\Psi_1(\\alpha_j+x_{ij}) - \\Psi_1(\\alpha_j)) -\\sum_{i = 1}^n\\Psi_1(|\\alpha|+|x_i|) + n\\Psi_1(|\\alpha|)$;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    " f(\\mathbf{x} \\mid \\alpha) \n",
    "&=\\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)} \\\\\n",
    "&= \\frac{|\\mathbf{x}|!}{\\prod_{j=1}^d x_j!}\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_logpdf(Array{T<:Any, 1}, Array{T<:Any, 1}) in module Main at In[806]:2 overwritten at In[809]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf (generic function with 2 methods)"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    tempt = lfact(sum(x)) + lgamma(sum(α)) - lgamma(sum(α) + sum(x))\n",
    "    for i = 1:size(x)[1]\n",
    "         tempt = tempt - lfact(x[i]) - lgamma(α[i]) + lgamma(x[i] + α[i])\n",
    "    end\n",
    "    return tempt\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_logpdf!(Array{T<:Any, 1}, Array{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[47]:2 overwritten at In[807]:2.\n",
      "WARNING: Method definition dirmult_logpdf(Array{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[47]:15 overwritten at In[807]:15.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'dirmult_logpdf :: Tuple{Array{T,2},Array{T,1}}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(dirmult_logpdf(data',ones(size(data,2)))) = -648774.1475893459\n",
      "sum(dirmult_logpdf(X',ones(size(X,2)))) = -625098.718876444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-625098.718876444"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download(\"http://hua-zhou.github.io/teaching/biostatm280-2017spring/hw/optdigits.tra\")\n",
    "data = readcsv(\"optdigits.tra\")\n",
    "\n",
    "#remove the last column (which digit it is) from the dataset\n",
    "X = data[:, 1:size(data, 2) .!= size(data, 2)]\n",
    "# remove columns which sum up to zero\n",
    "tempt = sum(X, 1)\n",
    "X = data[:, find(tempt)]\n",
    "\n",
    "# total likelihood without removing columns which sum up to zero\n",
    "@show sum(dirmult_logpdf(data', ones(size(data, 2))))\n",
    "\n",
    "# total likelihood after removing columns which sum up to zero\n",
    "@show sum(dirmult_logpdf(X', ones(size(X, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "We use the following notation in the subsequent discussion.\n",
    "$x_{ij},i = 1, ..., n, j = 1, ..., d$\n",
    "\n",
    "Here we use the definition of digamma and trigamma distribution:\n",
    "\n",
    "$\\Psi(x) = \\frac{d}{dx}log\\Gamma(x)$ and $\\Psi_{(1)}(x) = \\frac{d^2}{dx^2}log\\Gamma(x)$\n",
    "\n",
    "\n",
    "Score function:\n",
    "\n",
    "$\\nabla L(\\alpha) = \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n\\Psi(|\\alpha|+|x_i|)+n\\Psi(|\\alpha|)$\n",
    "\n",
    "Observed information matrix: \n",
    "\n",
    "For $k = j$, $-d^2L(\\alpha)_{j, k} = -\\sum_{i = 1}^n (\\Psi_1(\\alpha_j+x_{ij})-\\Psi_1(\\alpha_j))+\\sum_{i = 1}^n\\Psi_1(|\\alpha|+|x_i|) -n\\Psi_1(|\\alpha|)$;\n",
    "\n",
    "For $k \\neq j$, $-d^2L(\\alpha)_{j, k} = \\sum_{i = 1}^n\\Psi_1(|\\alpha|+|x_i|) -n\\Psi_1(|\\alpha|)$\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf{E}[-d^2L(\\alpha)]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since calculating $\\mathbf{E}[-d^2L(\\alpha)]$  is hard in our case, Fisher scoring method is not suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The observed information matrix can be written as the sum of a symmetric matix and a rank one matrix (matrix with one constant value). \n",
    "\n",
    "When calculating the inverse of the observed information matrix, consider the Woodbury formula again.\n",
    "$$\n",
    "\t(\\mathbf{A} + \\mathbf{U} \\mathbf{V}^T)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} (\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})^{-1} \\mathbf{V}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "Here $\\mathbf{A}$ is our diagonal matrix (name diagonal), $\\mathbf{U}$ is a d $\\times$1 with a constant value (name constant) in each entry, $\\mathbf{V^T}$ is a 1 $\\times$ d matrix with values 1. m = 1 in our case. \n",
    "\n",
    "The observed information matrix is not always zero in our case, here is the possible remedy.\n",
    "\n",
    "To make det($-d^2L(\\alpha)$) = det($\\mathbf{A}$) det(1 + constant * $\\sum_{i = 1}^d{a_{ii}})$ >0,\n",
    "we need to specify two criteria: (1) each element of the diagonal matrix $\\mathbf{A}$ is greater than 0; (2) constant > -$\\frac{1}{\\sum_{i = 1}^d{a_{ii}}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_invobs(Array{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[803]:4 overwritten at In[823]:4.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_invobs (generic function with 1 method)"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the inverse of observed information matrix\n",
    "# constant, diagnoal is defined as we discussed before\n",
    "function dirmult_invobs(X::Matrix, α::Vector)\n",
    "    constant = 0\n",
    "    n = size(X, 1)\n",
    "    d = size(X, 2)  \n",
    "    rowsums = sum(X, 2)  \n",
    "    for i in 1:n\n",
    "        constant = constant + trigamma(sum(α) + rowsums[i]) - \n",
    "        trigamma(sum(α))\n",
    "    end\n",
    "    @show constant\n",
    "    diagonal = zeros(d, d)  \n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            diagonal[j, j] = diagonal[j, j] - trigamma(α[j] + \n",
    "            X[i, j]) + trigamma(α[j])\n",
    "        end\n",
    "       # if diagonal[j, j] <= 0 \n",
    "        #    diagonal[j, j] = abs(diagonal[j, j]) + 0.1\n",
    "        #end\n",
    "    end\n",
    "    #diagonal = abs(diagonal) + 1\n",
    "\n",
    "    pd = isposdef(diagonal)\n",
    "  #  if constant <= - (1 / sum(diagonal))\n",
    "   #     constant = - (1 / sum(diagonal)) + 1\n",
    "   # end\n",
    "   # if sum(diagonal) <= - 1 / constant\n",
    "    #    diagonal = diagnoal + (sum(diagnoal) + 1 / constant) / d\n",
    "   # end\n",
    "    # define U and V according to the Woodbury Formula\n",
    "    U = ones(d, 1) * constant\n",
    "    V = ones(d, 1)\n",
    "    \n",
    "    inv_observed = inv(Diagonal(diagonal)) - inv(Diagonal(diagonal)) * U * \n",
    "    inv(1 + V' * inv(Diagonal(diagonal)) * U) * V' * inv(Diagonal(diagonal))\n",
    "    return inv_observed, pd, constant\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7\n",
    "Discuss how to choose a good starting point. Implement this as the default starting value in your function below. (Hint: Method of moment estimator may furnish a good starting point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the estimated $E(p_k)$ and $E(p_k^2)$ by going over the dataset (k = 1, ..., d; i = 1, ..., n; j = 1, ..., d):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{p}_{ij} &= \\frac{x_{ij}}{\\sum_{j=1}^n x_{ij}}\\\\\n",
    "E(p_k) &= \\frac{1}{n}\\sum_{i=1}^n p_{ik} \\\\\n",
    "E(p_k^2) &= \\frac{1}{n}\\sum_{i=1}^n p_{ik}^2\n",
    "\\end{align}\n",
    "$$\n",
    "We then calculate the first and second moments of $p_k$:\n",
    "$$\n",
    "\\begin{align}\n",
    "E(p_k) &= \\frac{\\alpha_k}{\\sum_{k=1}^d \\alpha_k}      ..........(1) \\\\\n",
    "E(p_k^2) &= E(p_k) \\frac{1+\\alpha_k}{1+\\sum_{k=1}^d \\alpha_k}\n",
    "\\end{align}\n",
    "$$\n",
    "Then we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{k=1}^d \\alpha_k = \\frac{E(p_k) - E(p_k^2)}{E(p_k^2) - E(p_k)^2} .........(2)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying expression (1) on both hand sides of (2), we get the initial values of each $\\alpha_k$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_k = \\frac{E(p_k) - E(p_k^2)}{E(p_k^2) - E(p_k)^2} E(p_k)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_initials(Array{T<:Any, 2}) in module Main at In[815]:4 overwritten at In[824]:4.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  9.13 MiB\n",
       "  allocs estimate:  1996\n",
       "  --------------\n",
       "  minimum time:     2.402 ms (0.00% GC)\n",
       "  median time:      4.372 ms (0.00% GC)\n",
       "  mean time:        5.188 ms (21.37% GC)\n",
       "  maximum time:     62.096 ms (53.65% GC)\n",
       "  --------------\n",
       "  samples:          956\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we implement a function to calculate the initial values\n",
    "# ep: e(pk) and e(pk^2)\n",
    "function dirmult_initials(X::Matrix)\n",
    "    rowsums = sum(X, 2)\n",
    "    n = size(X, 1)\n",
    "    d = size(X, 2)\n",
    "    ep = zeros(2, d)\n",
    "    alpha = zeros(d)\n",
    "    for j in 1:d\n",
    "        ep[1, j] = sum(X[:, j] ./ rowsums) / n\n",
    "        ep[2, j] = sum((X[:, j] ./ rowsums) .^2) / n\n",
    "        alpha[j] = (ep[1, j] - ep[2, j]) * ep[1, j] / (ep[2, j] - (ep[1, j]^2))\n",
    "    end\n",
    "    return alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using the Newton's method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_gradient(Array{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[825]:2 overwritten at In[826]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function dirmult_gradient(X::Matrix, α::Vector)\n",
    "n = size(X, 1)\n",
    "d = size(X, 2)   \n",
    "rowsums = sum(X, 2)\n",
    "sum_alpha = sum(α)\n",
    "    # evaluate gradient (score)\n",
    "        gradient = zeros(d)\n",
    "        for j in 1:d\n",
    "            for i in 1:n\n",
    "                gradient[j] = gradient[j] + digamma(α[j] + X[i,j]) - digamma(α[j]) -\n",
    "                digamma(sum_alpha + rowsums[i]) + digamma(sum_alpha) \n",
    "        end\n",
    "    end\n",
    "    return gradient\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_newton(Array{T<:Any, 2}) in module Main at In[830]:24 overwritten at In[831]:24.\n",
      "WARNING: Method definition #dirmult_newton(Array{Any, 1}, Main.#dirmult_newton, Array{T<:Any, 2}) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'dirmult_newton :: Tuple{Array{T,2}}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "function dirmult_newton(X::Matrix; α0::Vector = nothing, \n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6)\n",
    "    n = size(X, 1)\n",
    "    d = size(X, 2)\n",
    "    α = α0\n",
    "    iterations = 0\n",
    "    loglold = sum(dirmult_logpdf(X', α))    \n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "       # @show α[1:5]\n",
    "        \n",
    "        # evaluate gradient (score)\n",
    "        gradient = dirmult_gradient(X, α)\n",
    "        \n",
    "        # approximated the inverse of observed information matrix\n",
    "        results = dirmult_invobs(X, α)\n",
    "        inv_observed = results[1]   \n",
    "       # @show results[2]\n",
    "        \n",
    "        # compute Newton's direction\n",
    "        direction = inv_observed * gradient\n",
    "\n",
    "        # line search loop\n",
    "        #@show loglold\n",
    "        for lsiter in 1:10\n",
    "            step = 1 / (2^(lsiter - 1))\n",
    "            new_α = α + step * direction\n",
    "            logl = sum(dirmult_logpdf(X', new_α))\n",
    "            if(logl > loglold)\n",
    "                α = new_α\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        #@show logl\n",
    "        loglold = logl\n",
    "        # check convergence criterion \n",
    "       if abs(logl - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            iterations = iter\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    logl = sum(dirmult_logpdf(X', α))\n",
    "    gradient = dirmult_gradient(X, α)\n",
    "    inv_observed = dirmult_invobs(X, α)\n",
    "    hessian =  - inv(inv_observed)\n",
    "\n",
    "    return logl, α, gradient, hessian, iterations\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant = -31.762899479741726\n",
      "constant = -35425.34438602504\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching inv(::Tuple{Array{Float64,2},Bool,Float64})\u001b[0m\nClosest candidates are:\n  inv(\u001b[1m\u001b[31m::Complex{Float64}\u001b[0m) at complex.jl:255\n  inv(\u001b[1m\u001b[31m::Integer\u001b[0m) at int.jl:36\n  inv{T<:Integer}(\u001b[1m\u001b[31m::Complex{T<:Integer}\u001b[0m) at complex.jl:122\n  ...\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching inv(::Tuple{Array{Float64,2},Bool,Float64})\u001b[0m\nClosest candidates are:\n  inv(\u001b[1m\u001b[31m::Complex{Float64}\u001b[0m) at complex.jl:255\n  inv(\u001b[1m\u001b[31m::Integer\u001b[0m) at int.jl:36\n  inv{T<:Integer}(\u001b[1m\u001b[31m::Complex{T<:Integer}\u001b[0m) at complex.jl:122\n  ...\u001b[0m",
      "",
      " in #dirmult_newton#108(::Array{Float64,1}, ::Int64, ::Float64, ::Function, ::Array{Float64,2}) at ./In[827]:69",
      " in (::#kw##dirmult_newton)(::Array{Any,1}, ::#dirmult_newton, ::Array{Float64,2}) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "dirmult_newton(X, α0 = dirmult_initials(X))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant = -3.2558856353599945\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Base.LinAlg.SingularException(7)",
     "output_type": "error",
     "traceback": [
      "Base.LinAlg.SingularException(7)",
      "",
      " in inv(::Diagonal{Float64}) at ./linalg/diagonal.jl:274",
      " in dirmult_invobs(::Array{Float64,2}, ::Array{Float64,1}) at ./In[823]:36",
      " in #dirmult_newton#108(::Array{Float64,1}, ::Int64, ::Float64, ::Function, ::Array{Float64,2}) at ./In[827]:38",
      " in (::#kw##dirmult_newton)(::Array{Any,1}, ::#dirmult_newton, ::Array{Float64,2}) at ./<missing>:0",
      " in macro expansion; at ./In[829]:13 [inlined]",
      " in anonymous at ./<missing>:?"
     ]
    }
   ],
   "source": [
    "# subsetting the dataset by each digits\n",
    "α0 = dirmult_initials(X)\n",
    "alphas = zeros(10, size(X, 2))\n",
    "for digit in 0:9\n",
    "    digit  = 1\n",
    "    index = findn(data[:, size(data, 2)] .== digit)\n",
    "    # subsetting according to the digits\n",
    "    tempt = data[index, :]\n",
    "    # remove the columns that sum up to 0 as we did before\n",
    "    tempt = tempt[:, index_not0]\n",
    "    # remove the last column\n",
    "    tempt = tempt[:, 1:(size(tempt, 2) - 1)]\n",
    "    alphas[digit + 1, :] = dirmult_newton(tempt, α0 = α0)[2]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α[1:5] = [0.0985004,1.75236,8.54345,9.90779,1.05419]\n",
      "constant = -0.13517425185478532\n",
      "results[2] = true\n",
      "loglold = -177267.02564211818\n",
      "logl = -177325.5297782579\n",
      "constant = -0.13517425185478532\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching inv(::Tuple{Array{Float64,2},Bool,Float64})\u001b[0m\nClosest candidates are:\n  inv(\u001b[1m\u001b[31m::Complex{Float64}\u001b[0m) at complex.jl:255\n  inv(\u001b[1m\u001b[31m::Integer\u001b[0m) at int.jl:36\n  inv{T<:Integer}(\u001b[1m\u001b[31m::Complex{T<:Integer}\u001b[0m) at complex.jl:122\n  ...\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching inv(::Tuple{Array{Float64,2},Bool,Float64})\u001b[0m\nClosest candidates are:\n  inv(\u001b[1m\u001b[31m::Complex{Float64}\u001b[0m) at complex.jl:255\n  inv(\u001b[1m\u001b[31m::Integer\u001b[0m) at int.jl:36\n  inv{T<:Integer}(\u001b[1m\u001b[31m::Complex{T<:Integer}\u001b[0m) at complex.jl:122\n  ...\u001b[0m",
      "",
      " in #dirmult_newton#104(::Array{Float64,1}, ::Int64, ::Float64, ::Function, ::Array{Float64,2}) at ./In[791]:74",
      " in (::#kw##dirmult_newton)(::Array{Any,1}, ::#dirmult_newton, ::Array{Float64,2}) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "dirmult_newton(tempt, α0 = dirmult_initials(tempt))[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
