{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biostats 280 HW5\n",
    "### Wenxi Yu\n",
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: **\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution: **\n",
    "\n",
    "From the last homework, we know\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha) &= \\sum_{i=1}^n\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)] \\\\\n",
    "&= \\sum_{i=1}^n\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln\\frac{\\Gamma(\\alpha_j + x_{ij})}{\\Gamma(\\alpha_j)} - \\sum_{i=1}^n \\ln\\frac{\\Gamma(|\\alpha|+|\\mathbf{x}_i|) }{\\Gamma(|\\alpha|)} \\\\\n",
    "&= \\sum_{i=1}^n\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +  \\sum_{i=1}^n \\sum_{j=1}^d \\ln a_j(a_j + 1) \\cdots (a_j + x_{ij} - 1) - \\sum_{i=1}^n \\ln(|\\alpha|)(|\\alpha|+1)\\cdots (|\\alpha|+|\\mathbf{x}_i|-1) \\\\ \n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "**Question: **\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution: **\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 &= \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Differentiate with respect to $\\alpha_j$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 &= \\frac{d}{d\\alpha_j}\\left[\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\right] \\times \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} + \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\times \\frac{d}{d\\alpha_j}\\left[\\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\right] \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)\\prod_{j=1}^d \\Gamma(\\alpha_j)- \\Gamma(|\\alpha|)\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]^2}\\times \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} + \\left[\\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\ln(p_j)\\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\right] \\\\\n",
    "&= \\frac{  \\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}   \\prod_{j=1}^d \\Gamma(\\alpha_j)- \\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p} + \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}  - \\frac{\\left[\\prod_{j=1}^d \\Gamma(\\alpha_j)\\right]'}{\\prod_{j=1}^d \\Gamma(\\alpha_j)}+ \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}  - \\frac{1}{ \\Gamma(\\alpha_j)}+ \\mathbf{E}(\\ln P_j) \\\\\n",
    "&= \\Psi(|\\alpha|) -\\Psi(\\alpha_j)  + \\mathbf{E}(\\ln P_j) \\\\\n",
    "\\text{Thus}\\,\\,\\, \\mathbf{E}(\\ln P_j) &= \\Psi(\\alpha_j) - \\Psi(|\\alpha|).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "**Question:**\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ follows a Dirichlet distribution with parameters ($x_{1j} + \\alpha_j^{(t)}$, $\\ldots, x_{nj} + \\alpha_j^{(t)}$), then we have\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_{ij}) = \\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|).\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i(\\alpha|\\alpha^{(t)}) &= \\int_{\\Delta_d} f(\\mathbf{p}_1,\\ldots,\\mathbf{p}_n |x_{1j} + \\alpha_j^{(t)}, \\ldots, x_{nj} + \\alpha_j^{(t)})\\ln f(\\mathbf{x}_i, \\mathbf{p}_i, \\mathbf{\\alpha})\\, d\\mathbf{p}_i\\\\\n",
    "&=\\int_{\\Delta_d}f(\\mathbf{p}_1,\\ldots,\\mathbf{p}_n |x_{1j} + \\alpha_j^{(t)}, \\ldots, x_{nj} + \\alpha_j^{(t)})\\ln \\left[f(\\mathbf{x}_i|\\mathbf{p}_i)\\pi(\\mathbf{p}_i)  \\right]\\, d\\mathbf{p}_i \\\\\n",
    "&= \\int_{\\Delta_d} \\frac{\\Gamma(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(x_{ij} + \\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{x_{ij} + \\alpha_j^{(t)}-1}\\ln\\left[ \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\\prod_{j=1}^d p_{ij}^{x_j} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{\\alpha_j-1} \\right] \\, d\\mathbf{p}_i\\\\\n",
    "&= \\int_{\\Delta_d} \\frac{\\Gamma(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(x_{ij} + \\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{x_{ij} + \\alpha_j^{(t)}-1} \\left[\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\ln p_{ij}+\\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\right] \\, d\\mathbf{p}_i \\\\ \n",
    "&= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1) \\int_{\\Delta_d} \\left[ \\frac{\\Gamma(|\\mathbf{x}_i| + |\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(x_{ij}  + \\alpha_j^{(t)})} \\prod_{j=1}^d p_{ij}^{x_{ij} + \\alpha_j^{(t)}-1}\\ln p_{ij}\\right] \\, d\\mathbf{p}_i + \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\\\\n",
    "&= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\mathbf{E}(\\ln P_{ij})+ \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\\\\n",
    "&= \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|) \\right]+ \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j) \\\\\n",
    "Q(\\alpha|\\alpha^{(t)}) &= \\sum_{i =1}^n Q_i(\\alpha|\\alpha^{(t)})  \\\\\n",
    "&= \\sum_{i =1}^n \\left[\\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +\\sum_{j = 1}^d(x_{ij} +\\alpha_j -1)\\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|) \\right]+ \\ln\\Gamma(|\\alpha|) -\\sum_{j = 1}^d \\ln \\Gamma(\\alpha_j)\\right] \\\\\n",
    "&= \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "**Question:**\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "From Question 1, we know that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha) &= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k) \\\\\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "We consider the following two terms and we use the indicator function $\\mathbf{1}$ where takes value of 1 if the condition in the bracket is satisfied:\n",
    "\n",
    "We first define  $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$\n",
    "\n",
    "Review **Jensen's Inequality: **\n",
    "- for a convex function $f$, $f(tx_1+(1-t)x_2)\\leq tf(x_1)+(1-t)f(x_2)$\n",
    "- for a concave function $f$, $f(tx_1+(1-t)x_2)\\geq tf(x_1)+(1-t)f(x_2)$\n",
    "\n",
    "Review **support hyperplane inequality:**\n",
    "- for a convex function $f$, $f(x) \\geq f(x_0) +f'(x_0)(x-x_0)$\n",
    "- similar to first order Taylor expansion\n",
    "\n",
    "Then we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) &= \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{\\max_{i}x_{ij}-1} \\ln(\\alpha_j+k)  1_{\\{x_{ij} > k\\}}\\\\\n",
    "&= \\sum_{j=1}^d \\sum_{k=0}^{\\max_{i}x_{ij}-1} \\ln(\\alpha_j+k)  \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}\\\\\n",
    "&= \\sum_{j=1}^d \\sum_{k=0}^{\\max_{i}x_{ij}-1} \\ln(\\alpha_j+k)\\times s_{jk} \\\\ \n",
    "&=\\sum_{j=1}^d \\sum_{k=0}^{\\max_{i}x_{ij}-1}  s_{jk} \\times \\ln(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} +k}\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\alpha_j + \\frac{k}{\\alpha_j^{(t)}+k}\\frac{\\alpha_j^{(t)}+k}{k}k)\\\\\n",
    "&\\geq \\sum_{j=1}^d \\sum_{k=0}^{\\max_{i}x_{ij}-1}  s_{jk} \\times[\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} +k} \\ln(\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\alpha_j) + \\frac{k}{\\alpha_j^{(t)}+k} \\ln(\\alpha_j^{(t)}+k)] \\\\\n",
    "&= \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c_1^{(t)} \\\\\n",
    "- \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k) &=-\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1}\\ln(|\\alpha|+k) \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}} \\\\\n",
    "&=-\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1}\\ln(|\\alpha|+k)r_k \\\\\n",
    "&\\geq -\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1} r_k [\\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}+ \\ln(|\\alpha^{(t)}|+k)] \\\\\n",
    "&= -\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1} r_k \\frac{|\\alpha|}{|\\alpha^{(t)}|+k} +c_2^{(t)} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha) &= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k) \\\\\n",
    "&\\geq \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j -\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1} r_k \\frac{|\\alpha|}{|\\alpha^{(t)}|+k} +c^{(t)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is the minoring function $g(\\alpha|\\alpha^{(t)})$.\n",
    "\n",
    "$g(\\alpha|\\alpha^{(t)}) = \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j -\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1} r_k \\frac{|\\alpha|}{|\\alpha^{(t)}|+k} +c^{(t)}$\n",
    "\n",
    "Since $\\alpha^{(t+1)} = \\text{argmin}\\, g(\\alpha|\\alpha^{(t)})$, we calculate the derivative of $g(\\alpha|\\alpha^{(t)})$ with respect to each $\\alpha_j$, that is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Let} \\, \\frac{dg(\\alpha|\\alpha^{(t)})}{d\\alpha_j} &=  \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\frac{1}{\\alpha_j}-\\sum_{k=0}^{\\max_i|\\mathbf{x}_i|-1} r_k \\frac{1}{|\\alpha^{(t)}|+k} = 0,\\\\\n",
    "\\text{we have} \\,\\,\n",
    "\\alpha_j^{(t+1)} &= \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  550k  100  550k    0     0  1760k      0 --:--:-- --:--:-- --:--:-- 1804k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  258k  100  258k    0     0   703k      0 --:--:-- --:--:-- --:--:--  708k\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "SystemError: opening file optdigits.tra: No such file or directory",
     "output_type": "error",
     "traceback": [
      "SystemError: opening file optdigits.tra: No such file or directory",
      "",
      " in #systemerror#51 at ./error.jl:34 [inlined]",
      " in systemerror(::String, ::Bool) at ./error.jl:34",
      " in open(::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./iostream.jl:89",
      " in open(::Base.#readstring, ::String) at ./iostream.jl:111",
      " in #readdlm_auto#11(::Array{Any,1}, ::Function, ::String, ::Char, ::Type{T}, ::Char, ::Bool) at ./datafmt.jl:126",
      " in #readdlm#8(::Array{Any,1}, ::Function, ::String, ::Char, ::Type{T}, ::Char) at ./datafmt.jl:106",
      " in #readdlm#4(::Array{Any,1}, ::Function, ::String, ::Char, ::Type{T}) at ./datafmt.jl:46",
      " in #readcsv#15(::Array{Any,1}, ::Function, ::String, ::Type{T}) at ./datafmt.jl:626",
      " in readcsv(::String, ::Type{T}) at ./datafmt.jl:626"
     ]
    }
   ],
   "source": [
    "# download file if it's not in current folder\n",
    "if !isfile(\"optdigits.tra\")\n",
    "    download(\"http://hua-zhou.github.io/teaching/\" * \n",
    "        \"biostatm280-2017spring/hw/optdigits.tra\")\n",
    "end\n",
    "if !isfile(\"optdigits.tes\")\n",
    "    download(\"http://hua-zhou.github.io/teaching/\" * \n",
    "        \"biostatm280-2017spring/hw/optdigits.tes\")\n",
    "end\n",
    "\n",
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "digit = traindata[:, end]\n",
    "count = traindata[:, 1:64]' # make sure each column is a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mom"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the code copied from hw4 sketch\n",
    "\"\"\"\n",
    "    dirmult_mom(X)\n",
    "\n",
    "Find the method of moment estimator of Dirichlet-multinomial distribution.\n",
    "Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_mom{T <: Real}(X::Matrix{T})\n",
    "    d, n   = size(X)\n",
    "    α      = zeros(Float64, d)\n",
    "    pi1    = zeros(Float64, d)\n",
    "    pi2    = zeros(Float64, d)\n",
    "    total  = zero(T)\n",
    "    for j in 1:n\n",
    "        colsum = zero(T)\n",
    "        for i in 1:d\n",
    "            α[i]   += X[i, j] # accumulate row sum\n",
    "            colsum += X[i, j]\n",
    "        end\n",
    "        total += colsum\n",
    "        if colsum > 0\n",
    "            for i in 1:d\n",
    "                pi      = X[i, j] / colsum\n",
    "                pi1[i] += pi   # accumulate pi\n",
    "                pi2[i] += pi^2 # accumulate pi^2\n",
    "            end  \n",
    "        end\n",
    "    end\n",
    "    # estiamte ρ and p\n",
    "    ρ = zero(Float64)\n",
    "    for i in 1:d\n",
    "        if pi1[i] > 0\n",
    "            ρ += pi2[i] / pi1[i]\n",
    "        end\n",
    "        α[i] /= total\n",
    "    end\n",
    "    αsum = max((d - ρ) / (ρ - 1), 1e-6)\n",
    "    for i in 1:d\n",
    "        α[i] *= αsum\n",
    "    end\n",
    "    return α\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x, α)\n",
    "\n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::AbstractVector, α::Vector)\n",
    "    T = promote_type(eltype(x), eltype(α))\n",
    "    xs = sum(x)\n",
    "    αs = sum(α)\n",
    "    if xs == 0 && αs == 0\n",
    "        return zero(T)\n",
    "    elseif xs > 0 && αs == 0\n",
    "        return convert(T, -Inf)\n",
    "    else\n",
    "        l = lfact(xs) - lgamma(xs + αs) + lgamma(αs)\n",
    "    end\n",
    "    for i in eachindex(x)\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return convert(T, -Inf)\n",
    "        elseif α[i] > 0\n",
    "            l += - lfact(x[i]) + lgamma(x[i] + α[i]) - lgamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    return l\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at a sample `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::AbstractMatrix, α::Vector)\n",
    "    l  = zero(promote_type(eltype(X), eltype(α)))\n",
    "    for j in 1:size(X, 2)\n",
    "        l += dirmult_logpdf(view(X, :, j), α)\n",
    "    end\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score! (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_score(X, α)\n",
    "\n",
    "Evaluate the score (gradient) of Dirichlet-multinomial log-likelihood at `α`.\n",
    "Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_score(X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    ∇ = zeros(T, length(α))\n",
    "    return dirmult_score!(∇, X, α)\n",
    "end\n",
    "\n",
    "function dirmult_score!(∇::Vector, X::AbstractMatrix, α::Vector)\n",
    "    fill!(∇, zero(eltype(∇)))\n",
    "    for j in 1:size(X, 2)\n",
    "        dirmult_score!(∇, view(X, :, j), α)\n",
    "    end\n",
    "    return ∇\n",
    "end\n",
    "\n",
    "function dirmult_score!(\n",
    "        ∇::Vector, \n",
    "        x::AbstractVector, \n",
    "        α::Vector\n",
    "    )\n",
    "    \n",
    "    T = promote_type(eltype(x), eltype(α))\n",
    "    xs = zero(eltype(x))\n",
    "    αs = zero(eltype(α))\n",
    "    for i in eachindex(x)\n",
    "        xs += x[i]\n",
    "        αs += α[i]\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            ∇[i] += Inf\n",
    "        elseif α[i] > 0\n",
    "            ∇[i] += digamma(x[i] + α[i]) - digamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    if αs > 0\n",
    "        c = digamma(xs + αs) - digamma(αs)\n",
    "        for i in eachindex(x)\n",
    "            ∇[i] -= c\n",
    "        end\n",
    "    end\n",
    "    return ∇\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obsinfo! (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_obsinfo(X, α)\n",
    "\n",
    "Evaluate the observed information matrix of Dirichlet-multinomial log-likelihood\n",
    "at `α`. Each column of `X` is one data point. Return vector `d` and constant `c`.\n",
    "The observed information matrix equals `Diagonal(d) - c`.\n",
    "\"\"\"\n",
    "function dirmult_obsinfo(X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    d = zeros(T, length(α))\n",
    "    return dirmult_obsinfo!(d, X, α)\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(d::Vector, X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    c = zero(T)\n",
    "    fill!(d, zero(eltype(d)))\n",
    "    for j in 1:size(X, 2)\n",
    "        c += dirmult_obsinfo!(d, view(X, :, j), α)\n",
    "    end\n",
    "    return c, d\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(d::Vector, x::AbstractVector, α::Vector)\n",
    "    T  = promote_type(eltype(x), eltype(α))\n",
    "    xs = zero(eltype(x))\n",
    "    αs = zero(eltype(α))\n",
    "    for i in eachindex(x)\n",
    "        xs += x[i]\n",
    "        αs += α[i]\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            d[i] += T(Inf)\n",
    "        elseif α[i] > 0\n",
    "            d[i] += T(trigamma(α[i]) - trigamma(x[i] + α[i]))\n",
    "        end\n",
    "    end\n",
    "    if αs == 0 && xs > 0\n",
    "        c = T(Inf)\n",
    "    elseif αs == 0 && xs == 0\n",
    "        c = zero(T)\n",
    "    elseif αs > 0\n",
    "        c = T(trigamma(αs) - trigamma(xs + αs))\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector = dirmult_mom(X), \n",
    "        maxiters::Int = 100, \n",
    "        tolfun = 1e-6,\n",
    "        verbose::Bool = true\n",
    "    )\n",
    "    # first transpose X\n",
    "    X = X'\n",
    "    \n",
    "    # only consider rows and columns with sum >0\n",
    "    rowind = find(sum(X, 2))\n",
    "    colind = find(sum(X, 1)) \n",
    "    Xwork = @view X[rowind, colind]\n",
    "    αwork  = α0[colind]\n",
    "\n",
    "    # preparation\n",
    "    αnew = similar(αwork)\n",
    "    n, d = size(Xwork)\n",
    "    niter = zero(Int)\n",
    "    loglold = dirmult_logpdf(Xwork', αwork)\n",
    "    rowsums = sum(Xwork, 2)\n",
    "    logliter = zero(Float64)\n",
    "    colmax = maximum(Xwork, 1)\n",
    "\n",
    "    for iter in 1:maxiters\n",
    "        # first calculate the denominator\n",
    "        denom = 0\n",
    "        for k in 0:(maximum(rowsums) - 1)\n",
    "            r = sum(rowsums .> k)\n",
    "            denom = denom + r / (sum(αwork) + k)\n",
    "        end\n",
    "        \n",
    "        # then calculate the numerator\n",
    "        for j in 1:d\n",
    "            tempt = 0\n",
    "            if(colmax[j] >= 1)\n",
    "                for k in 0:(colmax[j] - 1)\n",
    "                    s = sum(Xwork[:, j] .> k)\n",
    "                    tempt = tempt + s / (αwork[j] + k)\n",
    "                end\n",
    "                αnew[j] = tempt / denom * αwork[j]\n",
    "            end\n",
    "        end\n",
    "        logliter = dirmult_logpdf(Xwork', αnew)\n",
    "\n",
    "        # print iterate log-L if requested\n",
    "        if verbose\n",
    "            println(\"iteration \", iter, \", logl = \", logliter)\n",
    "        end\n",
    "        \n",
    "        copy!(αwork, αnew)\n",
    "        if abs(logliter - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            niter = iter\n",
    "            break\n",
    "        end\n",
    "        loglold = logliter\n",
    "        niter = maxiters\n",
    "    end\n",
    "\n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    αfinal = zeros(eltype(α0), length(α0))\n",
    "    αfinal[colind] = αwork\n",
    "    \n",
    "    ∇final = zeros(eltype(α0), length(α0))\n",
    "    ∇final[colind] = dirmult_score(Xwork', αwork)\n",
    "\n",
    "    obsinfo = zeros(eltype(α0), length(α0), length(α0))\n",
    "    obsinfo_dinv = similar(αwork)\n",
    "    obsinfo_c, obsinfo_d = dirmult_obsinfo!(obsinfo_dinv, Xwork', αwork)\n",
    "    obsinfo[colind, colind] = diagm(obsinfo_d) - obsinfo_c\n",
    "     \n",
    "    return logliter, niter, αfinal, ∇final, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-472076.27017447987,34,[0.0,0.080946,0.896003,2.10578,2.02127,0.765058,0.146442,0.0151446,0.00038966,0.301588  …  0.460085,0.027251,0.000129835,0.0654667,0.920936,2.11521,1.94414,0.946319,0.23171,0.0277971],[0.0,-4.02053,-6.93991,-8.16969,-8.14505,-6.97781,-4.74195,-3.91225,-3.81489,-5.24598  …  -6.35534,-3.93511,-3.81317,-4.02897,-7.11329,-8.22943,-8.31595,-7.5905,-5.22983,-3.95832],\n",
       "[0.0 0.0 … 0.0 0.0; 0.0 90730.9 … -68.7395 -68.7395; … ; 0.0 -68.7395 … 25188.0 -68.7395; 0.0 -68.7395 … -68.7395 2.68005e5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirmult_mm(count; verbose = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.794803271 seconds\n",
      "elapsed time: 0.383424724 seconds\n",
      "elapsed time: 0.046666279 seconds\n",
      "elapsed time: 0.4344732 seconds\n",
      "elapsed time: 0.202163888 seconds\n",
      "elapsed time: 0.282101987 seconds\n",
      "elapsed time: 0.11746834 seconds\n",
      "elapsed time: 0.045039027 seconds\n",
      "elapsed time: 0.187638052 seconds\n",
      "elapsed time: 0.333363963 seconds\n"
     ]
    }
   ],
   "source": [
    "using DataFrames, Distributions\n",
    "\n",
    "ndigit       = zeros(Int, 10)\n",
    "logl_dirmult = zeros(10)\n",
    "iter_dirmult = zeros(Int, 10)\n",
    "time_dirmult = zeros(10)\n",
    "αhat_dirmult = zeros(64, 10)\n",
    "logl_multnom = zeros(10)\n",
    "\n",
    "\n",
    "for d in 0:9\n",
    "    # retrieve data for digit d\n",
    "    Xd = traindata[traindata[:, end] .== d, 1:64]'\n",
    "    ndigit[d + 1] = size(Xd, 2)\n",
    "    # fit Dirichlet-multinomial\n",
    "    tic()\n",
    "    logl_dirmult[d + 1], iter_dirmult[d + 1], αhat, = dirmult_mm(Xd; verbose = false)\n",
    "    αhat_dirmult[:, d + 1] = αhat\n",
    "    time_dirmult[d + 1] = toc()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10×5 DataFrames.DataFrame\n",
      "│ Row │ digit │ n   │ logl_dm  │ iters │ runtime   │\n",
      "├─────┼───────┼─────┼──────────┼───────┼───────────┤\n",
      "│ 1   │ 0     │ 376 │ -37361.9 │ 100   │ 0.794803  │\n",
      "│ 2   │ 1     │ 389 │ -42179.5 │ 47    │ 0.383425  │\n",
      "│ 3   │ 2     │ 380 │ -39985.3 │ 6     │ 0.0466663 │\n",
      "│ 4   │ 3     │ 389 │ -40519.9 │ 48    │ 0.434473  │\n",
      "│ 5   │ 4     │ 387 │ -43489.0 │ 26    │ 0.202164  │\n",
      "│ 6   │ 5     │ 376 │ -41191.6 │ 36    │ 0.282102  │\n",
      "│ 7   │ 6     │ 377 │ -37703.0 │ 16    │ 0.117468  │\n",
      "│ 8   │ 7     │ 387 │ -40304.1 │ 6     │ 0.045039  │\n",
      "│ 9   │ 8     │ 380 │ -43131.3 │ 20    │ 0.187638  │\n",
      "│ 10  │ 9     │ 382 │ -43709.9 │ 39    │ 0.333364  │"
     ]
    }
   ],
   "source": [
    "result = DataFrame(digit = 0:9, n = ndigit, \n",
    "    logl_dm = logl_dirmult, \n",
    "    iters = iter_dirmult, runtime = time_dirmult)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Array{Int64,2}:\n",
       " 174    0    0    0    4    0    0    0    0    0\n",
       "   0  133   21    0    2    0    2    0   13   11\n",
       "   0    9  151    2    0    1    1    1    8    4\n",
       "   0    2    1  155    0    4    0    7    5    9\n",
       "   0    2    0    0  173    0    1    2    2    1\n",
       "   0    0    0    0    1  166    1    0    0   14\n",
       "   0    6    0    0    2    1  170    0    2    0\n",
       "   0    0    0    0   11    0    0  164    2    2\n",
       "   0   22    1    0    1    1    1    1  134   13\n",
       "   0    5    0    4    6    2    0    3    4  156"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLBase\n",
    "\n",
    "# read in test digits\n",
    "testdata = readcsv(\"optdigits.tes\", Int)\n",
    "Xtest    = testdata[:, 1:64]'\n",
    "ytest    = testdata[:, 65]\n",
    "\n",
    "# matrix of assignment probability\n",
    "test_digit_prob = [dirmult_logpdf(Xtest[:, j], αhat_dirmult[:, d]) \n",
    "    for d in 1:10, j in 1:size(Xtest, 2)]\n",
    "\n",
    "# factor in prior probabilities\n",
    "test_digit_prob .= test_digit_prob .+ log(ndigit / sum(ndigit))\n",
    "\n",
    "# prediction\n",
    "pred = [indmax(test_digit_prob[:, j]) for j in 1:size(Xtest, 2)]\n",
    "\n",
    "# confusion matrix\n",
    "confmat = confusmat(10, ytest + 1, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the efficacy of Newton's method and MM algorithm:\n",
    "- for one iteration, both of the methods take similar amount of time;\n",
    "- however for MM algorithm, it normally takes more iterations to converge to the MLE;\n",
    "- so Newton's method is much more efficient than MM algorithm in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "** Question:**\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution: **\n",
    "\n",
    "Consider the supporting hyperplane inequality: For a convex function $f(x)$, we have $f(x) \\geq f(x_0) + f'(x_0) (x-x_0)$. \n",
    "\n",
    "Since the function $z \\mapsto \\ln \\Gamma(z)$ is a convex function, we have\n",
    "\n",
    "$$\\ln \\Gamma(|\\alpha|) \\geq \\ln \\Gamma(|\\alpha^{(t)}|) + \\Psi(|\\alpha^{(t)}|) (|\\alpha| - |\\alpha^{(t)}|)$$\n",
    "\n",
    "So\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(\\alpha|\\alpha^{(t)}) &= \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)} \\\\\n",
    "&\\geq \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha^{(t)}|) +n \\Psi(|\\alpha^{(t)}|) (|\\alpha| - |\\alpha^{(t)}|) + c^{(t)} \\\\\n",
    "&= Q'(\\alpha|\\alpha^{(t)}) \\\\\n",
    "\\text{Take} \\, \\frac{dQ'(\\alpha|\\alpha^{(t)}) }{d\\alpha_j} &= \\sum_{i=1}^n  \\left[\\Psi(x_{ij}+\\alpha_j^{(t)})- \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\Psi(\\alpha_j)+n \\Psi(|\\alpha^{(t)}|) =0, \\\\\n",
    "\\end{aligned}\n",
    "$$ \n",
    "we can calculate $\\alpha_j$ according to the last iteration of $\\alpha_j$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
